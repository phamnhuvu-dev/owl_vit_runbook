{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9MKZb6G3-H92"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/big_vision/')\n",
    "from bokeh import io as bokeh_io\n",
    "import jax\n",
    "from google.colab import output as colab_output\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scenic.projects.owl_vit import configs\n",
    "from scenic.projects.owl_vit import models\n",
    "\n",
    "from scenic.projects.owl_vit.notebooks import inference\n",
    "from scenic.projects.owl_vit.notebooks import interactive\n",
    "from scenic.projects.owl_vit.notebooks import plotting\n",
    "from scipy.special import expit as sigmoid\n",
    "import skimage\n",
    "from skimage import io as skimage_io\n",
    "from skimage import transform as skimage_transform\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.experimental.set_visible_devices([], 'GPU')\n",
    "bokeh_io.output_notebook(hide_banner=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnD94y6ia6Mn"
   },
   "source": [
    "# Set up the model\n",
    "This takes a minute or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1UiX2Nx8auW4"
   },
   "outputs": [],
   "source": [
    "config = configs.clip_b16.get_config(init_mode='canonical_checkpoint')\n",
    "module = models.TextZeroShotDetectionModule(\n",
    "    body_configs=config.model.body,\n",
    "    normalize=config.model.normalize,\n",
    "    box_bias=config.model.box_bias)\n",
    "variables = module.load_variables(config.init_from.checkpoint_path)\n",
    "model = inference.Model(config, module, variables)\n",
    "model.warm_up()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0Kckjo-Z7nr"
   },
   "source": [
    "# Load example images\n",
    "\n",
    "Please provide a path to a directory containing example images. Google Cloud Storage and local storage are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmdvY7AEZ9dK"
   },
   "outputs": [],
   "source": [
    "IMAGE_DIR = 'gs://scenic-bucket/owl_vit/example_images'  # @param {\"type\": \"string\"}\n",
    "%matplotlib inline\n",
    "\n",
    "from skimage import data\n",
    "\n",
    "images = {}\n",
    "\n",
    "for i, filename in enumerate(tf.io.gfile.listdir(IMAGE_DIR)):\n",
    "  with tf.io.gfile.GFile(os.path.join(IMAGE_DIR, filename), 'rb') as f:\n",
    "    image = mpl.image.imread(\n",
    "        f, format=os.path.splitext(filename)[-1])[..., :3]\n",
    "  if np.max(image) <= 1.:\n",
    "    image *= 255\n",
    "  images[i] = image\n",
    "\n",
    "images[3] = data.rocket()\n",
    "images[4] = data.astronaut()\n",
    "\n",
    "cols = 5\n",
    "rows = max(len(images) // 5, 1)\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(16, 8 * rows))\n",
    "\n",
    "for ax in axs.ravel():\n",
    "  ax.set_visible(False)\n",
    "\n",
    "for ax, (ind, image) in zip(axs.ravel(), images.items()):\n",
    "  ax.set_visible(True)\n",
    "  ax.imshow(image)\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "  ax.set_title(f'Image ID: {ind}')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNzcyP1sbJ9w"
   },
   "source": [
    "# Text-conditioned detection\n",
    "Enter comma-separated queries int the text box above the image to detect stuff. If nothing happens, try running the cell first (<kbd>Ctrl</kbd>+<kbd>Enter</kbd>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "8teG83eKbNKl"
   },
   "outputs": [],
   "source": [
    "#@title { run: \"auto\" }\n",
    "IMAGE_ID =   4# @param {\"type\": \"number\"}\n",
    "image = images[IMAGE_ID]\n",
    "_, _, boxes = model.embed_image(image)\n",
    "plotting.create_text_conditional_figure(\n",
    "    image=model.preprocess_image(image), boxes=boxes, fig_size=900)\n",
    "interactive.register_text_input_callback(model, image, colab_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title { run: \"auto\" }\n",
    "IMAGE_ID =   2# @param {\"type\": \"number\"}\n",
    "image = images[IMAGE_ID]\n",
    "_, _, boxes = model.embed_image(image)\n",
    "plotting.create_text_conditional_figure(\n",
    "    image=model.preprocess_image(image), boxes=boxes, fig_size=900)\n",
    "interactive.register_text_input_callback(model, image, colab_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFlZhrDTQbiY"
   },
   "source": [
    "# Image-conditioned detection\n",
    "\n",
    "In image-conditioned detection, the model is tasked to detect objects that match a given example image. In the cell below, the example image is chosen by drawing a bounding box around an object in the left image. The model will then detect similar objects in the right image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQGAM16fReow"
   },
   "outputs": [],
   "source": [
    "#@title { run: \"auto\" }\n",
    "\n",
    "#@markdown The *query image* is used to select example objects:\n",
    "QUERY_IMAGE_ID = 3  # @param {\"type\": \"number\"}\n",
    "\n",
    "#@markdown Objects will be detected in the *target image* :\n",
    "TARGET_IMAGE_ID = 4  # @param {\"type\": \"number\"}\n",
    "\n",
    "#@markdown Threshold for the minimum confidence that a detection must have to\n",
    "#@markdown be displayed (higher values mean fewer boxes will be shown):\n",
    "MIN_CONFIDENCE = 0.9994 #@param { type: \"slider\", min: 0.9, max: 1.0, step: 0.0001}\n",
    "\n",
    "\n",
    "#@markdown Threshold for non-maximum suppression of overlapping boxes (higher\n",
    "#@markdown values mean more boxes will be shown):\n",
    "NMS_THRESHOLD = 0.15 #@param { type: \"slider\", min: 0.05, max: 1.0, step: 0.01}\n",
    "\n",
    "interactive.IMAGE_COND_MIN_CONF = MIN_CONFIDENCE\n",
    "interactive.IMAGE_COND_NMS_IOU_THRESHOLD = NMS_THRESHOLD\n",
    "\n",
    "query_image = images[QUERY_IMAGE_ID]\n",
    "target_image = images[TARGET_IMAGE_ID]\n",
    "_, _, boxes = model.embed_image(target_image)\n",
    "plotting.create_image_conditional_figure(\n",
    "    query_image=model.preprocess_image(query_image),\n",
    "    target_image=model.preprocess_image(target_image),\n",
    "    target_boxes=boxes, fig_size=600)\n",
    "interactive.register_box_selection_callback(model, query_image, target_image, colab_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title { run: \"auto\" }\n",
    "\n",
    "#@markdown The *query image* is used to select example objects:#@title { run: \"auto\" }\n",
    "\n",
    "#@markdown The *query image* is used to select example objects:\n",
    "QUERY_IMAGE_ID = 1  # @param {\"type\": \"number\"}\n",
    "\n",
    "#@markdown Objects will be detected in the *target image* :\n",
    "TARGET_IMAGE_ID = 0  # @param {\"type\": \"number\"}\n",
    "\n",
    "#@markdown Threshold for the minimum confidence that a detection must have to\n",
    "#@markdown be displayed (higher values mean fewer boxes will be shown):\n",
    "MIN_CONFIDENCE = 0.9994 #@param { type: \"slider\", min: 0.9, max: 1.0, step: 0.0001}\n",
    "\n",
    "\n",
    "#@markdown Threshold for non-maximum suppression of overlapping boxes (higher\n",
    "#@markdown values mean more boxes will be shown):\n",
    "NMS_THRESHOLD = 0.15 #@param { type: \"slider\", min: 0.05, max: 1.0, step: 0.01}\n",
    "\n",
    "interactive.IMAGE_COND_MIN_CONF = MIN_CONFIDENCE\n",
    "interactive.IMAGE_COND_NMS_IOU_THRESHOLD = NMS_THRESHOLD\n",
    "\n",
    "query_image = images[QUERY_IMAGE_ID]\n",
    "target_image = images[TARGET_IMAGE_ID]\n",
    "_, _, boxes = model.embed_image(target_image)\n",
    "plotting.create_image_conditional_figure(\n",
    "    query_image=model.preprocess_image(query_image),\n",
    "    target_image=model.preprocess_image(target_image),\n",
    "    target_boxes=boxes, fig_size=600)\n",
    "interactive.register_box_selection_callback(model, query_image, target_image, colab_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark inference speed\n",
    "- This section shows how to benchmark the inference speed of OWL-ViT. \n",
    "- Speed and accuracy can be traded off by reducing the input resolution. \n",
    "- This is done by truncating the position embeddings, and it works if the model was trained with heavy size augmentation and padding at the bottom and/or right of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configs.clip_b16.get_config(init_mode='canonical_checkpoint')\n",
    "\n",
    "# To use variable inference resolution, patch size and native (=training) grid\n",
    "# size need to be added to the config:\n",
    "config.model.body.patch_size = int(config.model.body.variant[-2:])\n",
    "config.model.body.native_image_grid_size = (\n",
    "    config.dataset_configs.input_size // config.model.body.patch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictWithTextEmbeddings(models.TextZeroShotDetectionModule):\n",
    "  \"\"\"Module that performs box prediction with precomputed query embeddings.\"\"\"\n",
    "\n",
    "  def __call__(self, image, query_embeddings):\n",
    "    feature_map = self.image_embedder(image[None, ...], False)  # Add batch dim.\n",
    "    b, h, w, d = feature_map.shape\n",
    "    image_features = feature_map.reshape(b, h * w, d)\n",
    "    boxes = self.box_predictor(\n",
    "        image_features=image_features, feature_map=feature_map\n",
    "    )['pred_boxes']\n",
    "    logits = self.class_predictor(image_features, query_embeddings[None, ...])[\n",
    "        'pred_logits'\n",
    "    ]\n",
    "    return boxes, logits\n",
    "\n",
    "\n",
    "module = PredictWithTextEmbeddings(\n",
    "    body_configs=config.model.body,\n",
    "    normalize=config.model.normalize,\n",
    "    box_bias=config.model.box_bias,\n",
    ")\n",
    "\n",
    "variables = module.load_variables(config.init_from.checkpoint_path)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def predict(image, query_embeddings):\n",
    "  return module.apply(variables, image, query_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Get fake query embeddings for benchmarking (1203 classes):\n",
    "embed_dim = models.clip_model.CONFIGS[config.model.body.variant]['embed_dim']\n",
    "query_embeddings = jax.random.normal(jax.random.PRNGKey(0), (1203, embed_dim))\n",
    "\n",
    "# Resolutions at which to benchmark the model:\n",
    "if config.model.body.patch_size == 16:\n",
    "  sizes = [100, 200, 368, 400, 448, 480, 528, 576, 624, 672, 736]\n",
    "else:\n",
    "  raise ValueError(\n",
    "      'Please define image sizes for patch size:'\n",
    "      f' {config.model.body.patch_size}'\n",
    "  )\n",
    "num_trials = 5\n",
    "all_timings = {}\n",
    "for image_size in sizes:\n",
    "  print(f'Benchmarking image size: {image_size}')\n",
    "\n",
    "  # Get fake image for benchmarking:\n",
    "  image = jax.random.uniform(jax.random.PRNGKey(0), (image_size, image_size, 3))\n",
    "  timings = []\n",
    "  for i in range(num_trials + 1):  # Add 1 trial to account for compilation.\n",
    "    start_time = time.time()\n",
    "    boxes, logits = predict(image, query_embeddings)\n",
    "    _ = jax.block_until_ready((boxes, logits))\n",
    "    timings.append(time.time() - start_time)\n",
    "\n",
    "  # Store the median. Note that the first trial will always be very slow due to\n",
    "  # model commpilation:\n",
    "  all_timings[image_size] = np.median(timings)\n",
    "  print(f'FPS at resolution={image_size}: {1/all_timings[image_size]:.2f}\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "OWL-ViT inference playground.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
